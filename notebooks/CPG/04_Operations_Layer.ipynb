{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "ML Ops is gaining a lot of popularity. This example showcases a key piece you can use to construct your automation pipeline. As we can see in the following architecture diagram, you will be deploying an AWS Step Funciton Workflow containing AWS Lambda functions that call Amazon S3, Amazon Personalize, and Amazon SNS APIs.\n",
    "\n",
    "\n",
    "This package contains the source code of a Step Functions pipeline that is able to perform \n",
    "multiple actions within **Amazon Personalize**, including the following:\n",
    "\n",
    "- Dataset Group creation\n",
    "- Datasets creation and import\n",
    "- Solution creation\n",
    "- Solution version creation\n",
    "- Campaign creation\n",
    "\n",
    "Once the steps are completed, the step functions notifies the users of its completion through the\n",
    "use of an SNS topic.\n",
    "\n",
    "The below diagram describes the architecture of the solution:\n",
    "\n",
    "![Architecture Diagram](../../static/imgs/ml_ops_architecture.png)\n",
    "\n",
    "The below diagram showcases the StepFunction workflow definition:\n",
    "\n",
    "![stepfunction definition](../../static/imgs/step_functions.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Installing AWS SAM\n",
    "\n",
    "The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.\n",
    "\n",
    "**Install** the [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html). \n",
    "This will install the necessary tools to build, deploy, and locally test your project. In this particular example we will be using AWS SAM to build and deploy only. For additional information please visit our [documentation](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html).\n",
    "\n",
    "\n",
    "**Note:** We have pre-installed SAM CLI in this notebooks through a cloudformation life cycle policy config\n",
    "\n",
    "Let's check what version of SAM we have installed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sam --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "Let's take a look at directory structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple artifacts that we will be using to build our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ml_ops/domain`**\n",
    "* This directory contains the configuration file and sample data based on the domain. In this example we are going to be using the Retail domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ml_ops/lambdas`**\n",
    "* This directory contains all the code that will be going into the lambda functions, these lambda functions will become a step inside the stepfunctions state machine we will deploy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ml_ops/template.yaml`**\n",
    "* This is our SAM template that will deploy the automation into our account, here we are printing just the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/template.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying\n",
    "\n",
    "In order to deploy the project you will need to run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/; sam build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/; sam deploy --template-file template.yaml --stack-name notebook-automation --capabilities CAPABILITY_IAM --s3-bucket $(aws cloudformation describe-stack-resources --stack-name AmazonPersonalizeImmersionDay --logical-resource-id SAMArtifactsBucket --query \"StackResources[0].PhysicalResourceId\" --output text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the bucket that our cloudformation deployed. We will be uploading our data to this bucket, plus the configuration file to trigger the automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = !aws cloudformation describe-stacks --stack-name notebook-automation --query \"Stacks[0].Outputs[?OutputKey=='InputBucketName'].OutputValue\" --output text\n",
    "bucket_name = bucket[0]\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the bucket name, lets copy over our Media data so we can explore and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R /home/ec2-user/SageMaker/amazon-personalize-immersion-day/automation/ml_ops/domain/CPG ./example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "import botocore\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from packaging import version\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup Clients\n",
    "\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "personalize_events = boto3.client('personalize-events')\n",
    "\n",
    "# We will upload our training data in these files:\n",
    "raw_items_filename = \"example/data/Items/items.csv\"                # Do Not Change\n",
    "raw_users_filename = \"example/data/Users/users.csv\"                # Do Not Change\n",
    "raw_interactions_filename = \"example/data/Interactions/interactions.csv\"  # Do Not Change\n",
    "items_filename = \"items.csv\"                # Do Not Change\n",
    "users_filename = \"users.csv\"                # Do Not Change\n",
    "interactions_filename = \"interactions.csv\"  # Do Not Change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.read_csv(raw_interactions_filename)\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways of uploading your datasets to S3:\n",
    "1. Using the boto3 SDK\n",
    "1. Using the CLI\n",
    "\n",
    "In this example we are going to use the CLI command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync ./example/data s3://$bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the State Machine Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute the MLOps pipeline we need to provide a parameters file that will tell our state machine which names and configurations we want in our Amazon Personalize deployment.\n",
    "\n",
    "We have prepared a parameters.json file, let's explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example/params.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "print(json.dumps(data, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameters file is set up to run at the beginning of this workshop. So let's modify a couple fields to make sure we are not overwritting our previous deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Groups\n",
    "data['datasetGroup']['name'] = 'notebook-automation'\n",
    " \n",
    "# Datasets \n",
    "data['datasets']['Interactions']['name'] = 'na-interactions-ds'\n",
    "data['datasets']['Users']['name'] = 'na-users-ds'\n",
    "data['datasets']['Items']['name'] = 'na-items-ds'\n",
    "\n",
    "# Solutions\n",
    "\n",
    "data['solutions']['personalizedRanking']['name'] = 'na-personalizedRankingCampaign'\n",
    "data['solutions']['sims']['name'] = 'na-simsCampaign'\n",
    "data['solutions']['userPersonalization']['name'] = 'na-userPersonalizationCampaign'\n",
    "\n",
    "# Campaigns\n",
    "\n",
    "data['campaigns']['personalizedRankingCampaign']['name'] = 'na-personalizedRankingCampaign'\n",
    "data['campaigns']['simsCampaign']['name'] = 'na-simsCampaign'\n",
    "data['campaigns']['userPersonalizationCampaign']['name'] = 'na-userPersonalizationCampaign'\n",
    "\n",
    "# Event Tracker\n",
    "\n",
    "data['eventTracker']['name'] = 'na-eventTracker'\n",
    "\n",
    "print(json.dumps(data, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating and uploading your parameters file to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's write the file locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example/params.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload this file to S3, we are going to be using the CLI to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./example/params.json s3://$bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the deployment\n",
    "\n",
    "So far we have deployed the automation required lets take a look at the stepfunctions execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('stepfunctions')\n",
    "stateMachineArn = !aws cloudformation describe-stacks --stack-name notebook-automation --query \"Stacks[0].Outputs[?OutputKey=='DeployStateMachineArn'].OutputValue\" --output text\n",
    "stateMachineArn= stateMachineArn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response = client.describe_state_machine(\n",
    "    stateMachineArn=stateMachineArn\n",
    ")\n",
    "print(json.dumps(describe_response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions_response = client.list_executions(\n",
    "    stateMachineArn=stateMachineArn,\n",
    "    statusFilter='SUCCEEDED',\n",
    "    maxResults=2\n",
    ")\n",
    "print(json.dumps(executions_response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the succeeded execution\n",
    "\n",
    "Once your step functions are done executing, you can list the executions and describe them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions_response = client.list_executions(\n",
    "    stateMachineArn=stateMachineArn,\n",
    "    statusFilter='SUCCEEDED',\n",
    "    maxResults=2\n",
    ")\n",
    "print(json.dumps(executions_response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_executions_response = client.describe_execution(\n",
    "    executionArn=executions_response['executions'][0]['executionArn']\n",
    ")\n",
    "print(json.dumps(describe_executions_response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the input that was delivered to the State Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, this is the input from our Parameters file we uploaded to S3. This input json was then passed to lambda functions in the state machine to utilize across Amazon Personalize APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json.loads(describe_executions_response['input']), indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the time stamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, this is the input from our Parameters file we uploaded to S3. This input json was then passed to lambda functions in the state machine to utilize across Amazon Personalize APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Date:\")\n",
    "print(json.dumps(describe_executions_response['startDate'], indent=4, sort_keys=True, default=str))\n",
    "print(\"Stop Date:\")\n",
    "print(json.dumps(describe_executions_response['stopDate'], indent=4, sort_keys=True, default=str))\n",
    "print(\"Elapsed Time: \")\n",
    "elapsed_time = describe_executions_response['stopDate'] - describe_executions_response['startDate']\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the automation around an hour with fourty minutes.\n",
    "\n",
    "If you are interested in deploying this example in your environment, visit our [Github Samples Page](https://github.com/aws-samples/amazon-personalize-samples/tree/master/next_steps/operations/ml_ops) to download the latest codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
